\section{Preliminaries}\label{sec:preliminaries}

\subsection{Hidden Markov Models}\label{subsec:hmm}
%definition of HMM
\glspl{hmm} were introduced by Baum and Petrie in 1966~\cite{levinson1983introduction}.
\gls{hmm} are a class of probabilistic graphical models that are widely used to model sequences of observations with underlying hidden states.
These models consist of two main components: observations and hidden states.
The observations are the visible data emitted by the model, while the hidden states represent the underlying process that generates these observations.
The objective of an \gls{hmm} is to infer the hidden states based on the observations.
\glspl{hmm} have applications in fields such as speech recognition~\cite{chavan2013overview}, bioinformatics~\cite{de2007hidden}, and natural language processing~\cite{murveit1990integrating}.

\begin{definition}[Hidden Markov Model]
    A Hidden Markov Model (HMM) is a tuple $\mathcal{M} = (S, \mathcal{L}, \mathscr{l}, \tau,  \pi)$, where:
    \begin{itemize}
        \item $S$ is a finite set of states.
        \item $\mathcal{L}$ is a finite set of labels.
        \item $\mathscr{l}: S \rightarrow D(\mathcal{L})$ is the emission function.
        \item $\tau: S \rightarrow D(S)$ is the transition function.
        \item $\pi: S \rightarrow \mathbb{R}$ is the initial distribution.
    \end{itemize}
\end{definition}

Here, $D()$ denotes the set of probability distributions over a finite set. The model emits a label $l$ in state $s$ with probability $\mathscr{l}(s, l)$, transitions between states with probability $\tau(s, s')$, and starts in state $s$ with probability $\pi(s)$.

%HMM as matrices
\subsection{Matrix Representation of HMMs}\label{subsec:matrix-representation}
\glspl{hmm} can be represented using matrices.
The emission function $\mathscr{l}$ can be represented as a matrix $\omega$ where $\omega_{s, l} = \mathscr{l}(s, l)$.
The matrix $\omega$ has the size $|S| \times |\mathcal{L}|$.
The sum of each row in the matrix $\omega$ is equal to one, reflecting the total probability of emitting all labels from a given state.
\[
\omega = \begin{bmatrix}
    \mathscr{l}(s_1, l_1) & \cdots & \mathscr{l}(s_1, l_{|\mathcal{L}|}) \\
    \vdots & \ddots & \vdots \\
    \mathscr{l}(s_{|S|}, l_1)  & \cdots & \mathscr{l}(s_{|S|}, l_{|\mathcal{L}|})
\end{bmatrix}
\]
The transition function $\tau$ can be represented as a matrix $P$ where $P_{s, s'} = \tau(s, s')$.
The matrix $P$ has the size $|S| \times |S|$.
The sum of each row in $P$ is equal to one, reflecting the total probability of transitioning from a given state to all other states.
\[
P = \begin{bmatrix}
    \tau(s_1, s_1) &  \cdots & \tau(s_1, s_{|S|}) \\
    \vdots  & \ddots & \vdots \\
    \tau(s_{|S|}, s_1) & \cdots & \tau(s_{|S|}, s_{|S|})
\end{bmatrix}
\]
The initial distribution $\pi$ can be represented as a vector $\pi$ where $\pi_s = \pi(s)$.
The vector $\pi$ has the size $|S|$.
The sum of all elements in $\pi$ is equal to one, reflecting the total probability of starting in each state.
\[
\pi = \begin{bmatrix}
    \pi(s_1) \\
    \vdots \\
    \pi(s_{|S|})
\end{bmatrix} 
\]

\subsection{Observations and Hidden States}\label{subsec:observations-hidden-states}
An \gls{hmm} operates on sequences of observations, denoted as $O = {O_1, O_2, \ldots, O_N}$, where each $O_i$ is a sequence of labels $o_1, o_2, \ldots, o_{|\textbf{O}|-1}$.
The task is to infer the sequence of hidden states $S = s_1, s_2, \ldots, s_{|\textbf{O}|-1}$ that most likely generated these observations.

Given an observation sequence $O$, the goal is to maximize the probability of the hidden states conditioned on the observations:

This inference is commonly achieved using the Baum-Welch algorithm.

%Baum-Welch for HMM mathamatically
\subsection{Baum-Welch Algorithm}\label{subsec:baum-welch}
The Baum-Welch algorithm is a key method for estimating the parameters of an \gls{hmm} from observed data. It leverages the Expectation-Maximization (EM) framework and consists of two iterative steps:

\begin{enumerate}
    \item \textbf{Expectation Step (E-step)}: Compute the expected the forward and backward variables, for each state $s$ and time $t$. of the latent variables, which are the unobserved state sequences corresponding to the observations. These variables represent the likelihood of being in state $s$ at time $t$ given the observed data up to time $t$ and the likelihood of observing the remaining data from time $t$ onwards given the state $s$ at time $t$, respectivly. 
    \item \textbf{Maximization Step (M-step)}: Update the model parameters (emission matrix $\omega$, transition matrix $P$, and initial distribution $\pi$) to maximize the likelihood of the observed data, using the expected values computed in the E-step.
    \item Repeat the E-step and M-step until convergence.
\end{enumerate}


The Baum-Welch algorithm is particularly useful for estimating the properbilities of the emission and transition matrices of a HMM, given a set of observations, without knowing the hidden states that generated the observations.

Given a multiset of observations $\mathcal{O}$ and initial parameters $\textbf{x}_0$, the Baum-Welch algorithm estimates the parameters of a \gls{hmm} $\mathcal{P}$ by iteratively improving the current hypothesis $\textbf{x}_n$ using the previous estimate $\textbf{x}_{n-1}$ until a convergence criterion is met.
A hypothesis refers to a specific set of values for the parameters $\mathbf{x}$.

Each iteration of the algorithm produces a new hypothesis, denoted as $\textbf{x}_n$, which is the algorithm's current best guess for the parameter values based on the observed data.
The algorithm consists of three main steps: the forward-backward procedure, the update step, and the convergence criterion.
The Baum-Welch algorithm iteratively refines the parameters until the improvement between successive iterations falls below a predefined threshold.
This is typically evaluated using a convergence criterion such as:

\begin{equation}
    ||l(\textbf{x}_n) - l(\textbf{x}_{n-1})|| < \epsilon\label{eq:convergence-criterion}
\end{equation}

where $\epsilon > 0$ is a small threshold, and $l(\textbf{x}_n)$ denotes the likelihood of the observed data given the parameter values at the $n$-th iteration.

The algorithm stops when the change in parameters is sufficiently small, indicating that the model has converged to a local maximum of the likelihood function.
The parameter estimation procedure is outlined in \autoref{alg:parameter-estimation}.

\begin{algorithm}[htb!]
    \begin{codebox}
        \Procname{$\proc{Estimate-Parameters}(\mathcal{P}, \mathbf{x}_0, \mathcal{O})$}
        \li $\mathbf{x} \gets \mathbf{x}0$
        \li \While $\neg\proc{Criterion}(\mathbf{x}_{n-1}, \mathbf{x}_n)$
        \li \Do $\mathbf{x}_{n - 1} \gets \mathbf{x}_n$
        \li $(\alpha, \beta) = \proc{Forward-Backward}(\mathcal{P}(\mathbf{x}_n), \mathcal{O})$
        \li $\mathbf{x}_n = \proc{Update}(\mathcal{P}(\mathbf{x}_n), \mathcal{O}, \alpha, \beta)$ \End
        \li \Return $\mathbf{x}_n$
    \end{codebox}
    \caption{Parameter estimation procedure~\cite{p7}.}
    \label{alg:parameter-estimation}
\end{algorithm}

Starting with initial parameters $\mathbf{x}_0$, the parameter estimation procedure iteratively improves the current hypothesis $\mathbf{x}_n$ using the previous estimate $\mathbf{x}_{n-1}$ until a specified criterion for convergence is met.
The specifics of the $\proc{Forward-Backward}$ and $\proc{Update}$ procedures are detailed in \autoref{subsec:forward-backwards_algorithm} and \autoref{subsec:update-algorithm} from~\cite{p7}.%TODO: Add reference to the paper

\subsection{Forward-Backward Algorithm}\label{subsec:forward-backwards_algorithm}
%Forward-Backward algorithm


%Update of HMM
\subsection{Update Algorithm}\label{subsec:update-algorithm}
%Baum-Welch with matrix operations
