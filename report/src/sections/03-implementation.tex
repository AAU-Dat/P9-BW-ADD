\section{Implementation}\label{sec:implementation}
In this section, we will discuss the implementation of the project.
We will start by discussing the tools used in the implementation, followed by the transition from matrices to \glspl{add}.
Finally, we will discuss the implementation of the matrix operations using \glspl{add}.

\subsection{CUDD}\label{subsec:cudd}
The Colorado University Decision Diagram (CUDD) library~\cite{somenzi1997cudd} is a powerful tool for implementing and manipulating decision diagrams, including \glspl{bdd} and \glspl{add}. \glspl{add} are compact representations of functions, often used to handle large state spaces symbolically and efficiently.

In this project, the CUDD library stores \glspl{add} and performs operations on them.
Its optimized algorithms and efficient memory management allow us to handle large and complex matrices symbolically, leading to significant performance improvements over traditional methods.

The CUDD library is implemented in C, ensuring high-performance execution, but it also ensures it can be used in C++ programs.

\subsection{Storm}\label{subsec:storm}
Storm is a versatile, open-source probabilistic model checking tool designed to verify the correctness and properties of stochastic models~\cite{hensel2021probabilistic}. It supports a wide range of probabilistic models, including \glspl{hmm},\glspl{mc} and \glspl{mdp}. Storm allows users to analyze models efficiently by computing various quantitative properties, such as probabilities, expected rewards, or long-run averages.

A key feature of Storm is its ability to represent models symbolically, leveraging data structures like \glspl{bdd} and \glspl{add}. These symbolic representations compactly encode the model's state space and transition structure, enabling efficient manipulation and analysis even for large-scale systems. Storm achieves this by interfacing with the CUDD library, mentioned earlier.

In our implementation, Storm serves as a parser for loading the input models. Specifically, we utilize Storm to convert the model into its \gls{add} representation. This \gls{add} representation provides a compact and hierarchical encoding of the underlying matrices, which can then be used to perform symbolic matrix operations using the CUDD library.

The reason for using Storm lies in it is open-source, which makes it easy to integrate into our project. Storm is designed to handle large and complex models efficiently for model checking.
Therefore the next step in Storm is to calculate the parameters of interest, such as transition probabilities, rewards, or other metrics derived from the model. By performing these computations symbolically within the ADD framework, we achieve a scalable and efficient approach to analyzing stochastic models.

\subsection{Transition to ADDs}\label{subsec:transition-to-adds}
The first step in the implementation is to transition from vectors and matrices to \glspl{add}.
This conversion leverages the compact and efficient representation of \glspl{add} to perform operations symbolically.

To convert a vector into an \gls{add}, the vector must first be interpreted as a square matrix.
This step ensures compatibility with the \gls{add} representation, which organizes data hierarchically.
When a matrix is represented as an \gls{add}, the matrix also has to be square, as the \gls{add} representation requires a square matrix, if the matrix is not square, it has to be padded with zeros to make it square.

Consider the following vector:

\[
    V=
    \begin{bmatrix}
        1 & 2 & 3 & 4 \\
    \end{bmatrix}
\]

This vector corresponds to a matrix of size $4 \times 4$.

\[
    \begin{bmatrix}
        1 & 2 & 3 & 4 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
    \end{bmatrix}
\]
In an ADD, each layer corresponds to one binary variable (or bit) in the encoding of an index. 
For a matrix of size $n \times n$, where $n = 2^k$, the binary representation of the row and column indices requires $k$ bits each. 
By interleaving these bits (e.g., alternating between row and column bits), we construct a balanced and regular structure that preserves the matrix's two-dimensional nature.
In the case of the vector V, the vector has 4 elements, so it requires $4 = 2^2$ bits to represent the indices.

The binary representation of the vector entries is shown in \autoref{tab:vector}, the rest of the matrix indices is filled with zeros.

\begin{table}
    \centering
    \caption{Binary encoding of a vector V of size 4}
    \label{tab:vector}
    \begin{tabular}{lll}
        \toprule
        Vector Index & Value & Binary Encoding \\
        \midrule
        1            & 1     & 0000            \\
        2            & 2     & 0001            \\
        3            & 3     & 0010            \\
        4            & 4     & 0011            \\
        \bottomrule
    \end{tabular}
\end{table}

The \gls{add} representation of this vector is shown in \autoref{fig:add}.
The binary encodings determine the structure of the decision diagram, where each entry in the vector is stored as a terminal node.
The paths to these nodes are dictated by the binary representation of their indices.

\begin{figure*}
    \centering
    \input{figures/vector_add_example}
    \caption{Vector V represented as an ADD}
    \label{fig:add}
\end{figure*}

The conversion of a matrix to an \gls{add} is similar to that of a vector, but with an additional layer of nodes to represent the rows.
The \gls{add} can however be reduced as shown in \autoref{fig:add_reduced}.
This reduction is done by removing the duplicated terminal nodes, removing the redundant nodes and merging the nodes with the same children.
The techniques for reducing \glspl{add} is the standard reduction techniques used for \glspl{bdd}.
The reduction of the \gls{add} is done to reduce the size of the \gls{add} and to make the operations on the \gls{add} more efficient.
CUDD has built-in functions for reducing the \gls{add}, that follows the standard reduction techniques.

\begin{figure}
    \centering
    \input{figures/vector_add_example_reduced}
    \caption{Reduced ADD of matrix V}
    \label{fig:add_reduced}
\end{figure}

\subsection{Matrix operations using ADDs}\label{subsec:matrix-operations-using-adds}
The matrix operations are implemented using \glspl{add}.
The matrix operations implemented are matrix transpose, matrix addition, matrix multiplication, Hadamard product, Hadamard division, Kronecker product and Khatri-Rao product.

\[
    A = \begin{bmatrix}
            1 & 2 \\
            3 & 4 \\
    \end{bmatrix}
\]

and

\[
    B = \begin{bmatrix}
            5 & 6 \\
            7 & 8 \\
    \end{bmatrix}
\]

are used as examples in the following subsections.

\subsubsection{Matrix Transpose}
The matrix transpose is implemented by swapping the row and column variables in the \gls{add}. Specifically, for each path in the \gls{add} representing an entry $(i, j)$, the roles of the row index 
$i$ and column index $j$ are exchanged. The terminal nodes (values of the matrix entries) remain unchanged.
The transpose of matrix $A$ is:
\[
    A^T = \begin{bmatrix}
              1 & 3 \\
              2 & 4 \\
    \end{bmatrix}
\]

\subsubsection{Matrix addition}
Matrix addition is implemented by adding the terminal nodes of two \glspl{add} while keeping the structure of the row and column indices consistent. The process involves:
\begin{enumerate}
    \item Traversing the paths of both \glspl{add} simultaneously.
    \item Summing the values at the terminal nodes where the row and column indices match.
\end{enumerate}
The resulting \gls{add} represents the element-wise sum of the two matrices.
The sum of matrices $A$ and $B$ is:
\[
    A + B = \begin{bmatrix}
        6  & 8  \\
        10 & 12 \\
    \end{bmatrix}
\]

\subsubsection{Matrix multiplication}
Matrix multiplication is implemented symbolically using the dot product of the row and column indices. In the \gls{add}:
\begin{enumerate}
    \item For each pair of rows in the first matrix and columns in the second matrix, the corresponding elements are multiplied.
    \item The products are summed along the shared index, combining them into the final terminal nodes of the resulting \gls{add}.
\end{enumerate}
The hierarchical structure of the \gls{add} ensures that only relevant paths are explored, making the operation efficient.
The product of matrices $A$ and $B$ is
\[
    A \times B = \begin{bmatrix}
        19 & 22 \\
        43 & 50 \\
    \end{bmatrix}
\]

\subsubsection{Hadamard product}\label{subsubsec:hadamard-product}
The Hadamard product is implemented by pairwise multiplication of corresponding terminal nodes in the two \glspl{add}. For each matching row-column index pair $(i, j)$:
\begin{enumerate}
    \item The values from both \glspl{add} are multiplied.
    \item The resulting product is stored in the terminal node of the new \gls{add}.
\end{enumerate}
The structure of the indices remains unchanged.
The Hadamard product of matrices $A$ and $B$ is:

\[
    A \circ B = \begin{bmatrix}
        5  & 12 \\
        21 & 32 \\
    \end{bmatrix}
\]

\subsubsection{Hadamard division}
The Hadamard division is implemented as Hadamard product, but with division instead of multiplication. See~\autoref{subsubsec:hadamard-product} for more details.
The Hadamard division of matrices $A$ and $B$ is

\[
    A \oslash B = \begin{bmatrix}
        0.2    & 0.3333 \\
        0.4286 & 0.5    \\
    \end{bmatrix}
\]

\subsubsection{Kronecker product}
The Kronecker product is implemented by expanding the indices and terminal nodes of the two matrices symbolically, with the resulting \gls{add} having the dimensions of the sum of the dimensions of the two matrices. 
The Kronecker product is a generalization of the outer product, where each element of the first matrix is multiplied by the second matrix as a whole.
For each entry $(i, j)$ in the first matrix with value $a$, the second matrix $B$ is multiplied by $a$, and the indices are adjusted:
\begin{enumerate}
    \item The row and column indices of $B$ are shifted based on $i$ and $j$ of $A$.
    \item The resulting values are stored in a new \gls{add}.
\end{enumerate}
The Kronecker product of matrices $A$ and $B$ is

\[
    A \otimes B = \begin{bmatrix}
        5  & 6  & 10 & 12 \\
        7  & 8  & 14 & 16 \\
        15 & 18 & 20 & 24 \\
        21 & 24 & 28 & 32 \\
    \end{bmatrix}
\]

\subsubsection{Khatri-Rao product}
The Khatri-Rao product is implemented by combining rows of the first matrix with corresponding rows of the second matrix. For each row index $i$:
\begin{enumerate}
    \item The elements of row $i$ in the first matrix are multiplied element-wise with the entire row $i$ in the second matrix.
    \item The resulting row is constructed symbolically within the \gls{add}.   
\end{enumerate}
The Khatri-Rao product of matrices $A$ and $B$ is

\[
    A \bullet B = \begin{bmatrix}
        5  & 6  & 10 & 12 \\
        21 & 24 & 28 & 32 \\
    \end{bmatrix}
\]
The resulting \gls{add} has the dimensions of sum of the dimensions of the two matrices, as in the Kronecker product.

% \subsubsection{Matrix scalar}
% Matrix scalar is implemented by multiplying each element in the matrix by a scalar value.
% The scaling of matrix $A$ by a factor of 2 is
% \[
% 2 \times A = 2 \times \begin{bmatrix}
%     1 & 2 \\
%     3 & 4 \\
% \end{bmatrix} = \begin{bmatrix}
%     2 & 4 \\
%     6 & 8 \\
% \end{bmatrix}
% \]

% The \gls{add} representation of the scaled matrix is shown in Figure \ref{fig:add_scaling}.
% \begin{figure}
%     \centering
%     \begin{tikzpicture}[
% level 1/.style={sibling distance=20mm},
% level 2/.style={sibling distance=10mm},
% level 3/.style={sibling distance=10mm}
% ]
% \node[c] {$x_1$}
%     child{ node[c]  {$y_1$} edge from parent[zeroarrow]
%             child{ node[t] {2} 
%             }
%             child{ node [t] {4} edge from parent[onearrow]
%             }
%     }
%     child{ node[c] {$y_1$} edge from parent[onearrow]
%         child{ node[t] {6} edge from parent[zeroarrow]
%         }
%         child{ node[t] {8} edge from parent[onearrow]}
%     }
% ;
%     \end{tikzpicture}
%     \caption{Scaling of matrix A by a factor of 2}
%     \label{fig:add_scaling}
% \end{figure}
