\section{Implementation}\label{sec:implementation}
In this section, we will discuss the implementation of the project.
We will start by discussing \glspl{add} and their advantages and disadvantages.
How we transition from vectors and matrices to \glspl{add} will be discussed.
We will then discuss the CUDD library and the Storm model checker.
Finally, we will discuss the implementation of the matrix operations using \glspl{add}.


\subsection{Algebraic Decision Diagrams}\label{subsec:algebraic-decision-diagrams}
\acrfullpl{add} generalize \glspl{bdd}~\cite{bahar1997algebric}.
Unlike \glspl{bdd} which are limited binary values (\{0,1\})~\cite{bryant1986graph}, \glspl{add} extend this framework by allowing terminal nodes to store arbitrary values, such as integers or real numbers. 
This flexibility makes them well-suited for applications like probabilistic model checking, optimization, and symbolic matrix operations.

In an \gls{add}, each path from the root to a terminal node represents a unique variable assignment, with terminal nodes storing the associated function values.
Their hierarchical structure supports compact, symbolic representations and efficient manipulation of large functions.

Advantages of \glspl{add}:
\begin{itemize}
\item Compact Representation: \glspl{add} exploit redundancies in data, providing a compact representation for many functions. Shared subgraphs reduce memory usage, especially when storing large, structured data, such as matrices.
\item Canonical Form: For a fixed variable ordering, \glspl{add} are canonical. This property simplifies equivalence checking and ensures consistency in symbolic manipulations.
\item Efficient Operations: Operations such as addition and multiplication can be performed directly on \glspl{add}, often avoiding explicit enumeration of the underlying data.
\end{itemize}


Disadvantages of \glspl{add}:
\begin{itemize}
\item Variable Ordering Sensitivity: The size and efficiency of \glspl{add} depend heavily on the chosen variable ordering. A suboptimal ordering can lead to exponentially larger diagrams, reducing performance.
\item Overhead in Construction: Constructing an \gls{add} involves significant overhead compared to simpler data structures like arrays or hash tables, particularly for small datasets, where the benefits of compactness may not be realized.
\item Complexity for Certain Functions: While \glspl{add} work well for structured data, highly unstructured or random data can result in large, inefficient diagrams. In such cases, other data structures may be more suitable.
\end{itemize}

\glspl{add} can symbolically encode row and column indices as binary variables, enabling efficient storage and manipulation of matrix data.
This approach is particularly advantageous for sparse or structured matrices, where patterns and redundancies can be exploited.

In this work, \glspl{add} provide a foundation for symbolic matrix operations like addition, multiplication, and Kronecker product. Their compact representation and efficiency make them a powerful tool for handling complex, parameterized computations.

\subsection{Transition to ADDs}\label{subsec:transition-to-adds}
The first step in the implementation is to transition from vectors and matrices to \glspl{add}.
This conversion leverages the compact and efficient representation of \glspl{add} to perform operations symbolically.

To convert a vector into an \gls{add}, we encode the indices as binary variables and the values as terminal nodes.
It is important to note that all the variables in the \gls{add} has to be consistent, that is the number of variables in the \gls{add} has to be the same for all the nodes in the \gls{add}.

When a matrix is represented as an \gls{add}, the matrix has to be square, as the \gls{add} representation requires a square matrix, if the matrix is not square, it has to be padded with zeros to make it square.
This is because in our implementation, we use the CUDD library, which requires the matrices to be square to be represented as \glspl{add}.

\subsubsection{Vector to ADD}
We will illustrate the conversion from a vector to an \gls{add} using an example.
Consider the following vector:

\[
    V=
    \begin{bmatrix}
        1 & 2 & 3 & 4 \\
    \end{bmatrix}
\]

In an \gls{add}, each layer corresponds to one binary variable (or bit) in the encoding of an index. 
For a vector of size $n$, where $n = 2^k$, the binary representation of the column indices requires $k$ bits each. 
In the case of the vector $V$, the vector has 4 elements, so it requires $4 = 2^2$ bits to represent the indices.

We can create the binary representation of the indices by using the binary representation of the numbers from 0 to $k$.

The binary representation of the vector $V$ entries is shown in \autoref{tab:vector}.
\begin{table}
    \centering
    \caption{Binary encoding of a vector V of size 4}
    \label{tab:vector}
    \begin{tabular}{lll}
        \toprule
        Vector Index & Value & Binary Encoding \\
        \midrule
        0            & 1     & 00            \\
        1            & 2     & 01            \\
        2            & 3     & 10            \\
        3            & 4     & 11            \\
        \bottomrule
    \end{tabular}
\end{table}

The binary encodings determine the structure of the decision diagram, where each entry in the vector is stored as a terminal node.
The paths to these nodes are dictated by the binary representation of their indices.
The root node represents the first bit of the binary encoding, with subsequent layers corresponding to the remaining bits.

The \gls{add} representation of the vector $V$ is shown in \autoref{fig:add_vector}.
For matrices represented as \glspl{add}, the structure is similar to vectors, but with two sets of binary variables for row and column indices.
The terminal nodes store the matrix entries, with paths determined by the binary representation of the row and column indices.
The \gls{add} variables are interleaved, with the row and column indices alternating in the path from the root to the terminal nodes.
This is the way the CUDD library represents matrices as \glspl{add}.
\begin{figure}
    \centering
    \input{figures/vector_add_example_reduced}
    \caption{\gls{add} representation of a vector V of size 4}
    \label{fig:add_vector}
\end{figure}

\subsection{CUDD}\label{subsec:cudd}
The Colorado University Decision Diagram (CUDD) library~\cite{somenzi1997cudd} is a powerful tool for implementing and manipulating decision diagrams, including \glspl{bdd} and \glspl{add}. \glspl{add} are compact representations of functions, often used to handle large state spaces symbolically and efficiently.

In this project, the CUDD library stores \glspl{add} and performs operations on them.
Its optimized algorithms and efficient memory management allow us to handle large and complex matrices symbolically, leading to significant performance improvements over traditional methods.

The CUDD library is implemented in C, ensuring high-performance execution, but it also ensures it can be used in C++ programs.

\subsection{Storm}\label{subsec:storm}

Storm is a versatile, open-source probabilistic model checking tool designed to verify the correctness and properties of stochastic models~\cite{hensel2021probabilistic}. 
It supports a wide range of probabilistic models, including \glspl{mc} and \glspl{mdp}.

It does not include \glspl{hmm} in its supported models, but a \gls{hmm} can be enoded as an \gls{mc}~\cite{rabiner1989tutorial}.
Storm allows users to analyze models efficiently by computing various quantitative properties, such as probabilities, expected rewards, or long-run averages.

The main purpose of this project is to provide data-driven modeling tools for formal verification.
The Storm model checker is widely recognized as a state-of-the-art verification tool for probabilistic models, known for its efficiency and scalability.
For this reason, in this project we have chosen integrate our symbolic implementation of the Baum Welch algorithm within Stom. 

Storm uses the CUDD library for the symbolic representation of models, which makes it a natural choice for our implementation.
This translate into working directly with the symbolic representation of the models provided by the parsers available in Storm that, in principle could be direcly used (off the shelf) by Storm for verification purposes.

\subsection{Matrix operations using ADDs}\label{subsec:matrix-operations-using-adds}
The matrix operations are implemented using \glspl{add}.
The matrix operations implemented are matrix transpose, matrix addition, matrix multiplication, Hadamard product, Hadamard division, Kronecker product and Khatri-Rao product.

The matrices:
\[
    A = \begin{bmatrix}
            1 & 2 \\
            3 & 4 \\
    \end{bmatrix}
\text{ and }
    B = \begin{bmatrix}
            5 & 6 \\
            7 & 8 \\
    \end{bmatrix}
\]
are used as examples in the following subsections.

In CUDD the \textit{DdManager} is used to manage the \glspl{add} and the operations on them.
The \textit{DdNode} is used to represent both the \glspl{add} and the variables in the \glspl{add}, that is the row and column indices in the \glspl{add}.

\subsubsection{Matrix Transpose}
The matrix transpose is implemented by swapping the row and column variables in the \gls{add}. Specifically, for each path in the \gls{add} representing an entry $(i, j)$, the roles of the row index 
$i$ and column index $j$ are exchanged. The terminal nodes (values of the matrix entries) remain unchanged.
The transpose of matrix $A$ is:
\[
    A^T = \begin{bmatrix}
              1 & 3 \\
              2 & 4 \\
    \end{bmatrix}
\]

In CUDD the function we use for transposing an \gls{add} is implemented as \textit{Cudd\_addSwapVariables(DdManager * dd, DdNode * f, DdNode ** x, DdNode ** y, int  n )}, where \textit{f} is the \gls{add} to be transposed, \textit{x} and \textit{y} are the set variables to be swapped and \textit{n} is the size of the variables to be swapped.

\subsubsection{Matrix addition}
Matrix addition is implemented by adding the terminal nodes of two \glspl{add} while keeping the structure of the row and column indices consistent. The process involves:
\begin{enumerate}
    \item Traversing the paths of both \glspl{add} simultaneously.
    \item Summing the values at the terminal nodes where the row and column indices match.
\end{enumerate}
The resulting \gls{add} represents the element-wise sum of the two matrices.
The sum of matrices $A$ and $B$ is:
\[
    A + B = \begin{bmatrix}
        6  & 8  \\
        10 & 12 \\
    \end{bmatrix}
\]

In CUDD the function we use for adding two \glspl{add} is implemented as \textit{Cudd\_addApply(DdManager * dd, Cudd\_addPlus(), DdNode * f, DdNode * g)}, where \textit{f} and \textit{g} are the two \glspl{add} to be added and \textit{Cudd\_addPlus()} is the function that is used to add the two \glspl{add}.

\subsubsection{Matrix multiplication}
Matrix multiplication is implemented symbolically using the dot product of the row and column indices. In the \gls{add}:
\begin{enumerate}
    \item For each pair of rows in the first matrix and columns in the second matrix, the corresponding elements are multiplied.
    \item The products are summed along the shared index, combining them into the final terminal nodes of the resulting \gls{add}.
\end{enumerate}
The hierarchical structure of the \gls{add} ensures that only relevant paths are explored, making the operation efficient.
The product of matrices $A$ and $B$ is
\[
    A \times B = \begin{bmatrix}
        19 & 22 \\
        43 & 50 \\
    \end{bmatrix}
\]

We use the function \textit{Cudd\_addMatrixMultiply(DdManager dd, DdNode A, DdNode B, DdNode **z, int nz)} in CUDD to multiply two \glspl{add}. 
The function takes two \glspl{add} \textit{A} and \textit{B} to be multiplied as input and returns a pointer to the resulting \gls{add}.
\textit{z} is the set of variables that are dependent on the columns in \textit{A} and the rows in \textit{B}.
\textit{A} is assumed to have the same number of columns as \textit{B} has rows, namely \textit{nz}.
In the implementation of the matrix multiplication, we needed to rename the variables in the \glspl{add} to do the matrix multiplication, namely \textit{A}s columns variables and \textit{B}s row variables, as CUDD requires the variables in the \glspl{add} to have the same name.

\subsubsection{Hadamard product}\label{subsubsec:hadamard-product}
The Hadamard product is implemented by pairwise multiplication of corresponding terminal nodes in the two \glspl{add}. For each matching row-column index pair $(i, j)$:
\begin{enumerate}
    \item The values from both \glspl{add} are multiplied.
    \item The resulting product is stored in the terminal node of the new \gls{add}.
\end{enumerate}
The structure of the indices remains unchanged.
The Hadamard product of matrices $A$ and $B$ is:
\[
    A \circ B = \begin{bmatrix}
        5  & 12 \\
        21 & 32 \\
    \end{bmatrix}
\]

In CUDD the function we use for Hadamard product is implemented as \textit{Cudd\_addApply(DdManager * dd, Cudd\_addTimes(), DdNode * f, DdNode * g)}, where \textit{f} and \textit{g} are the two \glspl{add} to be multiplied and \textit{Cudd\_addTimes()} is the function that is used to multiply the two \glspl{add} elementwise.

\subsubsection{Hadamard division}
The Hadamard division is implemented as Hadamard product, but with division instead of multiplication. See~\autoref{subsubsec:hadamard-product} for more details.
The Hadamard division of matrices $A$ and $B$ is:

\[
    A \oslash B = \begin{bmatrix}
        0.2    & 0.3333 \\
        0.4286 & 0.5    \\
    \end{bmatrix}
\]

The Hadamard division is implemented in CUDD by \textit{Cudd\_addApply(DdManager * dd, Cudd\_addDivide(), DdNode * f, DdNode * g)}, where \textit{f} and \textit{g} are the two \glspl{add} to be divided and \textit{Cudd\_addDivide()} is the function that is used to divide the two \glspl{add} elementwise.

\subsubsection{Kronecker product}
The Kronecker product is implemented by symbolically expanding the indices and terminal nodes of the two matrices.
This operation results in a new \gls{add} with dimensions equal to the sum of the dimensions of the input matrices. 
The Kronecker product generalizes the outer product, multiplying each element of the first matrix by the entire second matrix.

For a specific entry in the first matrix $A$ at row $i$ and column $j$, with value $a_{ij}$, the second matrix $B$ is scaled by $a_{ij}$, and its indices are adjusted as follows:
and its indices are adjusted as follows:
\begin{enumerate}
\item Multiply the values of $B$ by $a_{ij}$.
\item The row indices of $B$ are shifted by $i \times \text{rows}(B)$ in the new \gls{add}.
\item The column indices of $B$ are shifted by $j \times \text{columns}(B)$ in the new \gls{add}.
\item The scaled values are stored in the new \gls{add}.
\end{enumerate}

This operation is not natively supported in the CUDD library, so we implemented it explicitly as a custom function. 
The Kronecker product of matrices $A$ and $B$ is:
\[
    A \otimes B = \begin{bmatrix}
        5  & 6  & 10 & 12 \\
        7  & 8  & 14 & 16 \\
        15 & 18 & 20 & 24 \\
        21 & 24 & 28 & 32 \\
    \end{bmatrix}
\]

\subsubsection{Khatri-Rao product}
The Khatri-Rao product is implemented by combining rows of the first matrix with corresponding rows of the second matrix. For each row index $i$:
\begin{enumerate}
    \item The elements of row $i$ in the first matrix are multiplied element-wise with the entire row $i$ in the second matrix.
    \item The resulting row is constructed symbolically within the \gls{add}.   
\end{enumerate}
This operation is not natively supported in the CUDD library, so we implemented it explicitly as a custom function.
The Khatri-Rao product of matrices $A$ and $B$ is:
\[
    A \bullet B = \begin{bmatrix}
        5  & 6  & 10 & 12 \\
        21 & 24 & 28 & 32 \\
    \end{bmatrix}
\]
The resulting \gls{add} has the dimensions of sum of the dimensions of the two matrices, as in the Kronecker product.

% \subsubsection{Matrix scalar}
% Matrix scalar is implemented by multiplying each element in the matrix by a scalar value.
% The scaling of matrix $A$ by a factor of 2 is
% \[
% 2 \times A = \begin{bmatrix}
%     2 & 4 \\
%     6 & 8 \\
% \end{bmatrix}
% \]

