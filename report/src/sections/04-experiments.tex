\section{Experiments}\label{sec:experiments}
This section describes the experiment that would evaluate the proposed symbolic implementation of the Baum-Welch algorithm in CuPAAL.
The experiment was designed to compare CuPAAL with another implementation: Jajapy, which uses a recursive approach to the Baum-Welch algorithm.
The experiment would be conducted to answer the following research question:

\begin{itemize}
\item \textbf{Question:} How does the scalability of CuPAALs symbolic implementation compare to Jajapy as the number of model states increases?
\end{itemize}

This question would evaluate the computational efficiency of the symbolic implementation in CuPAAL.
The insights gained would highlight the strengths and potential weaknesses of the symbolic approach to the Baum-Welch algorithm.

\subsection{Experimental Setup}
The experiment would evaluate how each implementation scales with increasing model states. 
We would conduct a single experiment to evaluate the proposed method.
The experiment was intended to be run on the machine with the specifications shown in \autoref{tab:machine-specs}. 
\begin{table}[htb!]
\centering
\caption{Machine Specifications from AAU Strato}
\label{tab:machine-specs}
    \begin{tabular}{ll}
        \toprule
        \textbf{Component} & \textbf{Specification} \\
        \midrule
        CPU & AMD EPYC 16-core \\
        RAM & 64 GB \\
        DISK & 50 GB \\
        \bottomrule
    \end{tabular}
\end{table}

The following implementations would used for comparison.

\begin{itemize}
\item \textbf{Jajapy}: A recursive implementation of the Baum-Welch algorithm~\cite{reynouard2023jajapy}.
\item \textbf{CuPAAL}: The fully symbolic implementation proposed in this work.
\end{itemize}

The chosen implementations are either fully recursive to fully symbolic approaches, providing a comprehensive comparison of methodologies.

\subsection{Experiment Dataset}
The experiment would use synthetic datasets; The dataset would have five different labels \{"A", "B", "C", "D", "E"\} and the same observation sequence for all runs of the experiment. 
We would use one observation sequence of length 20, as our implementation is not set up to handle multiple observation sequences.
The number of states in the model would be varied from 20 to 1020, with increments of 100 states.

The initial model would be generated with random parameters, and the observation sequence would be the same for all experiment runs.
The model would start in the same initial state, generating the transition and emission matrices randomly.
The emission matrix would be generated so that one state can emit a maximum of three different labels.

All implementations would tested on the same dataset to ensure fair comparison.

\subsection{Experimental Procedure}
The experiment would assess how the runtime of each implementation changes as the number of states in a synthetic model incrementally increases from 20 to 1020. 
The process would be done as follows:

\begin{enumerate}
\item Load the initial model and observation sequence.
\item Use each implementation of the Baum-Welch algorithm to estimate model parameters and record the runtime.
\item Increase the number of states in the model and load new initial parameters.
\item Repeat steps 2-3 for each implementation until the maximum number of states is reached.
\end{enumerate}

A link to the specific experiment can be found in\footnote{\url{https://github.com/AAU-Dat/P7-sudd/blob/P9-experiments/scaling_experiment.py}}. 

Scalability would be evaluated by examining runtime trends as model size increases.
This would provide insight into how each implementation scales with larger models and is critical for understanding the practical applicability of symbolic versus recursive-based implementations.

Results would be visualized using plots to illustrate trends and highlight differences in scalability between implementations.

\subsection{Discussion of Metrics and Methods}
The runtime would be the primary evaluation metric, as it directly reflects the computational efficiency of each implementation when handling larger models. 
By plotting runtime alongside the number of states, we would aim to identify patterns and scalability limits for each approach.

We wanted to track the implementations' memory consumption, as this is a critical factor in the Baum-Welch algorithm's scalability, and symbolic approaches are expected to be more memory-efficient.
However, tracking memory consumption is challenging and is therefore left for future work.
Instead, we focus on runtime as the primary metric, as it provides a clear and comparable measure of computational efficiency.